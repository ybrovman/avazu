<HTML>
<HEAD>
<TITLE>Kaggle Competition: Avazu Click-Through Rate Prediction</TITLE>
</HEAD>

<BODY>
<CENTER>
<H1>Kaggle Competition: Avazu Click-Through Rate Prediction</H1>
<H3>by Yuri M. Brovman, PhD</H3>
<table width = "800">
	<tr><td>
		<CENTER>

<p align="left"><b>Goal: </b>The goal of the competition was to predict whether an ad impression would produce a click from the user. The evaluation metric was minimizaton of the logarithmic loss.</p>

<p align="left"><b>Data/Code: </b>Here is the link to the actual <a href="http://www.kaggle.com/c/avazu-ctr-prediction">Avazu</a> Kaggle competition with all of the data and information. The code for my solution is on <a href="https://github.com/ybrovman/avazu">GitHub</a>.</p>

<p align="left"><b>Analysis: </b>Here is a basic explanation of my code and analysis that I used to solve this binary classification problem. I used Python with the scikit-learn library to perform most of the calculations. I hashed the values in each column taking the top 95% of values in order to avoid the long tail. I have compared the performance of several classifiers in the figures below. </p>

<p align="left"><b>Figure 1.</b> In order to understand which classifier to utilize for my predictions, it is important to understand computational limitations. This graph shows the model fitting time for several classifiers that I used, plotted as a function of the number of training examples. My machine has an Intel Core i7-2600K 3.4GHz CPU and 8GB RAM. Both the logistic regression and the Gaussian Naive Bayes classifiers were very fast, performing under 30 seconds for 5 million training examples. The support vector machine classifier (SVC) did not scale very well and I stopped testing after only 50,000 training examples. </p>

<img src="classifier_timing2.jpeg", width=600>

<p align="left"><b>Figure 2.</b> The cross validation set logloss error is presented here for several classifiers. I used 2 million training examples with a train / cross validation set split of 90% / 10%, respectively. Clearly, the logistic regression as well as the gradient boosting classifiers produced superior results.
</p>

<img src="classifier_performance2.jpeg", width=600>

<p align="left"><b>Figure 3.</b> The Receiver Operating Characteristic (ROC) curves for the same cross validation set used to generate Figure 2 are shown here. The area under the curve (AUC) is printed next to the classifier name in the legend.
</p>

<img src="ROC_curve2.jpeg", width=600>

<p align="left"><b>Conclusion: </b>The logistic regression classifier was very fast and produced good results. It was useful in order to quickly iterate over versions of the solution as I performed feature engineering and improved the code. The gradient boosting classifier produced the best overall results, at the expense of the long training time.
</p>

<p align="left"><b>Leaderboard: </b> My overall standing on the final <a href="http://www.kaggle.com/c/avazu-ctr-prediction/leaderboard">leaderboard</a> is 1246 / 1604 with a logloss score of 0.4229314.
</p>

</CENTER>
</td></tr>
</table>
</CENTER>

</BODY>

</HTML>